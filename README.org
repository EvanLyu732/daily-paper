#+TITLE: Evan's Daily Paper 
#+AUTHOR: Evan Lyu
#+auto_tangle: t
#+DESCRIPTION: Evan's Daily Paper
#+STARTUP: showeverything
#+STARTUP: align
#+OPTIONS: tocs:3

/I am *not* a researcher or scientist. Just a regular guy who follow my interest try to figure out some interesting stuff. Since I love emacs and org mode. Why not track all the papers that i read in a single org file? This is a lifelong repo./

* Content

| Date     | *Paper*                                                                                |
| [[20240423]] | A Survey of Embodied AI: From Simulators to Research Tasks                             |
| [[20240424]] | Why Functional Programming Matters                                                     |
| [[20240425]] | Recurrent Neural Networks (RNNs): A gentle Introduction and Overview                   |
| [[20240426]] | Neural Machine Translation by Jointly Learning to Align and Translate                  |
| [[20240427]] | A General Survey on Attention Mechanisms in Deep Learning                              |
| [[20240428]] | MEGALODON: Efficient LLM Pretraining and Inference with Unlimited Context Length       |
| [[20240429]] | Mega: Moving Average Equipped Gated Attention                                          |
| [[20240430]] | The Next Decade in AI                                                                  |
| [[20240501]] | The Bitter Lesson                                                                      |
| [[20240502]] | KAN: Kolmogorov–Arnold Networks                                                        |
| [[20240503]] | Multilayer feedforward networks are universal approximators                            |
| [[20240504]] | Sequence to Sequence Learning with Neural Networks                                     |
| [[20240505]] | Translating Videos to Natural Language Using Deep Recurrent Neural Networks            |
| [[20240506]] | Summarizing Source Code using a Neural Attention Model                                 |
| [[20240507]] | Learning to Control Fast-Weight Memories: An Alternative to Dynamic Recurrent Networks |
| [[20240508]] | Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention          |
| [[20240509]] | BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding       |
| [[20240510]] | Language Models are Unsupervised Multitask Learners                                    |
| [[20240511]] | Improving Language Understanding by Generative Pre-Training                            |
| [[20240512]] | Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond               |
| [[20240513]] | Cramming: Training a Language Model on a Single GPU in One Day                         |
| [[20240514]] | Autonomous LLM-driven research from data to human-verifiable research papers           |
| [[20240515]] | LoRA: Low-Rank Adaptation of Large Language Models                                     |
| [[20240516]] | When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively           |
| [[20240517]] | A PRIMER ON THE INNER WORKINGS OF TRANSFORMER-BASED LANGUAGE MODELS                    |
| [[20240518]] | Is artificial consciousness achievable? Lessons from the human brain                   |
| [[20240519]] | Teaching Algorithm Design: A Literature Review                                         |
| [[20240520]] | How Good Are Low-bit Quantized LLAMA3 Models? An Empirical Study                       |
| [[20240521]] | A Survey on Retrieval-Augmented Text Generation for Large Language Models              |
| [[20240522]] | Best Practices and Lessons Learned on Synthetic Data for Language Models               |
| [[20240523]] | Exploring the Limits of Language Modeling                                              |
| [[20240524]] | The First Law of Complexodynamics                                                      |
| [[20240525]] | The Unreasonable Effectiveness of Recurrent Neural Networks                            |
| [[20240526]] | Recurrent Models of Visual Attention                                                   |
| [[20240527]] | Neural Turing Machines                                                                 |
| [[20240528]] | Relational recurrent neural networks                                                   |
| [[20240529]] | Keeping Neural Networks Simple by Minimizing the Description Length of the Weights     |
|          |                                                                                        |
|          |                                                                                        |
|          |                                                                                        |
|          |                                                                                        |


-----
** 20240423
- Paper: A Survey of Embodied AI: From Simulators to Research Tasks
- Links: https://arxiv.org/pdf/2103.04918.pdf
- Ideas:
  1. Embodied AI Simulators: DeepMind Lab, AI2-THOR, SAPIEN, VirtualHome, VRKitchen, ThreeDWorld, CHALET, iGibson, and Habitat-Sim.

-----
** 20240424
- Paper: Why Functional Programming Matters
- Links: https://www.cs.kent.ac.uk/people/staff/dat/miranda/whyfp90.pdf
- Ideas:
  1. functional programming can improve modularization in an maintainable way
     using high-order function and lazy evaluation 

-----
** 20240425
- Paper: Recurrent Neural Networks (RNNs): A gentle Introduction and Overview 
- Links: https://arxiv.org/pdf/1912.05911.pdf
- Ideas:
  1. RNN deal with /sequence/ data.
  2. BPTT (Back Propagation Through Time): store weight when processed through each loss term
  3. LSTM (Long Short-Term Memory): design to handle vanish graident problems and introduce the /gated cell/ to store more information (*What information*?)
  4. DRNN (Deep Recurrent Neural Networks): stack ordinary RNN together.
  5. BRNN (Bidirectional Recurrent Neural Networks): /the authors create the section, but i do not get any ideas./
  6. Seq2Seq: /What problems does seq2seq or encoder-decoder structure solves?/
  7. Attention & Transformers: /Why Attentions works?/ /Why Skip-Connection works?/
  8. Pointer Networks

-----
** 20240426
- Paper: Neural Machine Translation by Jointly Learning to Align and Translate
- Links: https://arxiv.org/pdf/1409.0473
- Ideas:
  *What is the difference with encoder-decoder architecture?*
  1. this link may helps https://slds-lmu.github.io/seminar_nlp_ss20/attention-and-self-attention-for-nlp.html
  2. bidirectional RNN as encoder and a decoder that search through a sources sentences during translation. The architecture *lead to*
     a *attention* mechansim in the decoder.

-----
** 20240427
- paper: A General Survey on Attention Mechanisms in Deep Learning
- links: https://arxiv.org/pdf/2203.14263
- ideas:
  1. authors define a /task model/, which contains /four component/, 1. /the feature model/ 2. /the query model/ 3. /the attention model/ 4. /the output model/
  2. /feature model/: used to extract features can be RNN or CNN and ...., for turning o$Xn$ into $fn$
  3. /query model/: a /query/ tell which feature $fn$ to attend to.
  4. /attention model/: given input query $qn$ and features vectors $fn$, the model extract the key matrix $Kn$ and value matrix $Vn$ from $fn$. Traditionaly, this process can be achived by linear transformation and use weight matrix $Wk$ and $Wv$.
  5. /attention mechanisms/ can be classify into three categories: query-related, feature-related and general(not relate to query or feature).

 *To learn more about attention mechanisms, this page https://slds-lmu.github.io/seminar_nlp_ss20/attention-and-self-attention-for-nlp.html and 3blue1brown video https://www.3blue1brown.com/lessons/attentionare are helpful*

------
** 20240428
- paper: MEGALODON: Efficient LLM Pretraining and Inference with Unlimited Context Length
- links: https://arxiv.org/pdf/2404.08801
- ideas:
  1. traditional transformer: computation complexity, limited inductive bias.
  2. introduce the /complex exponential moving average(CEMA)/ components, timestamp normalization layer, normalized attention and pre-norm with two-hop residual configuraion.

     *Q1: This paper is based on the architecture of MEGA, But What is MEGA*?
     *Q2: Why this architecture and deal with unlimited length?*
     #+begin_quote
Evaluation on long-context modeling, including
perplexity in various context lengths up to 2M and long-context QA tasks in Scrolls (Parisotto et al.,

#+end_quote
*not understand*...
     
------
** 20240429
- paper: Mega: Moving Average Equipped Gated Attention
- links: https://arxiv.org/pdf/2209.10655
- ideas:
   1. sequence modeling common approaches: self-attention and EMA(exponential moving average)
     *Well, this kind of theortical paper is too difficult for me, mayme i should start with some basic ideas and understand the concepts by doing project.*
     
------
** 20240430
- paper: The Next Decade in AI
- links: https://arxiv.org/pdf/2002.06177
- ideas: 
  1. authors cites "The Bitter Lesson" - By Rich Sutton, i have seen this paper in many places. I should check out this paper.
  2. claim1: =/to build a robust, knowledge-driven approach to AI we must have the machinery of symbol-manipulation in our toolkit. Too much of useful knowledge is abstract to make do without tools that represent and manipulate abstraction, and to date, the only machinery that we know of that can manipulate such abstract knowledge reliably is the apparatus of symbol-manipulation/.=
  3. claim2: robust artificial intelligences properties:
     * have the ability to learn new knowledge
     * can learn knowledged that is symbolically represented.
     * significant knowledge is likely to be abstract.
     * rules and exceptions are co-existed
     * Some significant fraction of the knowledge that a robust system is likely to be causal, and to support counterfactuals.
     * Some small but important subset of human knowledge is likely to be innate; robust AI, too, should start with some important prior knowledge.
  4. claim3: rather than starting each new AI system from scratch, as a blank slate, with little knowledge of the world, we should seek to build learning systems that start with initial frameworks for domains like time, space, and causality, in order to speed up learning and massively constrain the hypothesis space.
  5. knowledge by itself it not enough. knowledge put into practice with tool of reasoning.
     #+begin_quote
a reasoning system that can leverage large-scale background knowledge
efficiently, even when available information is incomplete is a prerequisite to robustness.
#+end_quote

------
** 20240502
- paper: KAN: Kolmogorov–Arnold Networks
- links: https://arxiv.org/pdf/2404.19756
- ideas
  1. claim1: Kolmogorov-Arnold representation theorem
     *What is Kolmogorov-Arnold representation theorem? Why it can represented any function like Universal Approximation Theorem?* 
  2. claim2: MLP: learnable weights on edges, KAN learnable activation functions on edges.
     *TOMORRORW PAPER IS ABOUT UNIVERSAL APPROXIMATION THEOREM*
  3. claim3:  KANs’ nodes simply sum incoming signals without applying any non-linearities
  4. claim4:  KANs are nothing more than combinations of splines
     *What is splines?*
  5. claim5: Currently, the biggest bottleneck of KANs lies in its slow training. KANs are usually 10x slower than MLPs, given the same number of parameters. We should be honest that we did not try hard to optimize KANs’ efficiency though, so we deem KANs’ slow training more as an engineering problem to be improved in the future rather than a fundamental limitation. If one wants to train a model fast, one should use MLPs. In other cases, however, KANs should be comparable or better than MLPs, which makes them worth trying.  
------
------
** 20240503
- paper: Multilayer feedforward networks are universal approximators 
- links: https://cognitivemedium.com/magic_paper/assets/Hornik.pdf 
- ideas:
  1. claim1: Advocates of the virtues of multilayer feedfor- ward networks (e.g., Hecht-Nielsen, 1987) often cite /Kolmogorov’s/ (1957) superposition theorem or its more recent improvements (e.g.. Lorentz, 1976) in support of their capabilities. However, these results require a different unknown transformation (g in Lorentz’s notation) for each continuous function to be represented, while specifying an exact upper limit to the number of intermediate units needed for the representation.
  2. Anyway, this paper prove multilayer feedforward networks is a class of universal approximators.
     *While reading this paper, i am wondering why encoder-decoder structure network work? who proposed that? This is tomorrow topic.*


------
** 20240504
- paper: Sequence to Sequence Learning with Neural Networks 
- links: https://arxiv.org/pdf/1409.3215
- ideas:
  1. claim1:  DNNs can only be applied to problems whose inputs and targets can be sensibly encoded with vectors of fixed dimensionality.
  2. claim2:  network architecture,  one LSTM for encoder and another LSTM for decoder.
     *What is the encoder and the decoder has different network structure?*
------

** 20240505
- paper: Translating Videos to Natural Language Using Deep Recurrent Neural Networks 
- links: https://arxiv.org/pdf/1412.4729
- ideas:
  1. claim1: 
     ```
     video -> cnn -> lstm -> label
     ```

     *It seems like features extraction network is a kind of encoder-decoder structure networks.*
------

** 20240506
- paper: Summarizing Source Code using a Neural Attention Model 
- links: https://github.com/sriniiyer/codenn/blob/master/summarizing_source_code.pdf 
- ideas:
  1. claim1:
       dataset: stackoverflow that contains c# tag
       model: LSTM 

  *Today paper is about llm in code generation, i chose this paper from this slides https://webstanford.edu/class/cs224g/slides/Code%20Generation%20with%20LLMs.pdf and i discover ==Standford CS 224G=.= Great Resources for keeping track to frontier llm application.*


** 20240507
- paper: Learning to Control Fast-Weight Memories: An Alternative to Dynamic Recurrent Networks
- links: https://ieeexplore.ieee.org/document/6796337
- ideas:
  1. two feedword networks. first network produce "fast-weight" as short-term memory, memory controller 
     *Well, This URL is worth a look. https://people.idsia.ch//~juergen/most-cited-neural-nets.html*


** 20240508
- paper: Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention 
- links: https://arxiv.org/pdf/2006.16236
- ideas:
  1. claim1: tranditional transformers require quadratics memories, such as for $N$ input. The time complexity is $O(N_2)$. This paper propose linear transformation.
     *What's the difference between attention layer and self-attention layer?*
     #+begin_quote
every transformer can be seen as a recurrent neural network
#+end_quote




** 20240509
- paper: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
- links: https://arxiv.org/pdf/1810.04805 
- ideas:
  1. claim1: two methods for apply pre-trained language models to downstream tasks( feature-based and find-tuning)
     


** 20240510
- paper: Language Models are Unsupervised Multitask Learners
- links: https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf
- ideas:
  1. dataset WebText (millions of webpage)
  2. language modeling (unsupervised distribution estimation of examples, each example contains a length of symbols.)
        https://github.com/codelucas/newspaper
     


** 20240511
- paper: Improving Language Understanding by Generative Pre-Training
- links: https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf
- ideas:
  1. By pre-training on a diverse corpus with long stretches of contiguous text our model acquires significant world knowledge and ability to process long-range dependencies which are then successfully transferred to solving discriminative tasks such as question answering, semantic similarity assessment, entailmentdetermination, and text classification, improving the state of the art on 9 of the 12 datasets we study.

     *What sources of corpus is suitable for pre training?*



** 20240512
- paper: Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond
- links: https://arxiv.org/pdf/2304.13712
- ideas:
  *What's the difference between encoder-decoder structure with decoder only model?*
  1. decoder-only model
  2. nlu task: text classification, named entity recognition (NER),
entailment prediction, and so on. 
  3. nlg task: Natural Language Generation broadly encompasses two major categories of tasks, with the goal of creating coherent, meaningful, and contextually appropriate sequences of symbols. 
 

-------------
** 20240513
- paper: Cramming: Training a Language Model on a Single GPU in One Day
- links: https://arxiv.org/pdf/2212.14034
- ideas:
  no ideas here, i follow this https://magazine.sebastianraschka.com/p/understanding-large-language-models to read paper. But maybe one day this paper would be a good start for implementing BERT in consumer compute. I'm not sure if i will do this experiment. 


-------------
** 20240514
- paper: Autonomous LLM-driven research from data to human-verifiable research papers
- links: https://arxiv.org/pdf/2404.17605
- ideas:
  1. this paper propose data to paper. crazy idea...
  2. 
#+begin_quote
Starting with a human-provided dataset.the process is designed to raise hypotheses, write, debug and execute code to analyze the
data and perform statistical tests, interpret the results and write well-structured scientific
papers which not only describe results and conclusions but also transparently delineate the
research methodologies, allowing human scientists to understand, repeat and verify the
analysis. The discussion on emerging guidelines for AI-driven science (22) have served as a
design framework for data-to-paper, yielding a fully transparent, traceable and verifiable
workflow, and algorithmic \u201cchaining\u201d of data, methodology and result allowing to trace
downstream results back to the part of code which generated them. The system can run with
or without a predefined research goal (fixed/open-goal modalities) and with or without human
interactions and feedback (copilot/autopilot modes). We performed two open-goal and two
fixed-goal case studies on different public datasets (24\u201327) and evaluated the AI-driven
research process as well as the novelty and accuracy of created scientific papers. We show
that, running fully autonomously (autopilot), data-to-paper can perform complete and correct
run cycles for simple goals, while for complex goals, human co-piloting becomes critical.  
#+end_quote

-------------
** 20240515
- paper: LoRA: Low-Rank Adaptation of Large Language Models
- links: https://arxiv.org/pdf/2106.09685
- ideas:
  1. claim1: LoRA: which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable pa-rameters for downstream tasks


-------------
** 20240516
- paper: When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively
- links: https://arxiv.org/pdf/2404.19705
- ideas:
  1. claim1: this paper propose a method that when LLM generate token by <RET>, it use ir system for retrieve outer sources.


-------------
** 20240517
- paper: A PRIMER ON THE INNER WORKINGS OF TRANSFORMER-BASED LANGUAGE MODELS
- links: https://arxiv.org/pdf/2405.00208
- ideas:
  1. claim1: layer normalization is a common operation used to stabilize the training process of deep neural networks
     *This paper is too long for me to digest. Maybe one day i'll come to visit when i project transfromer architecture*

-------------
** 20240518
- paper: Is artificial consciousness achievable? Lessons from the
human brain
- links: https://arxiv.org/pdf/2405.04540
- ideas:
  1. claim1:
     #+begin_quote
Given this uncertainty, we recommend not to use the same general term (i.e., consciousness) for both humans and artificial systems; to clearly specify the key differences between them; and, last but not least, to be very clear about which dimension and level of consciousness the artificial system may possibly be capable of displaying.
#+end_quote

-------------
** 20240519
- paper: Teaching Algorithm Design: A Literature Review
- links: https://arxiv.org/pdf/2405.00832
- ideas:
  1. claim: *Systematic literature reviews*
     * Research Question
     * Protocol Development
     * Search Databases
     * Screen Studies
     * Extract Data
     * Assess Quality
     * Synthesize Data
     * Report Findings


-------------
** 20240520
- paper: How Good Are Low-bit Quantized LLAMA3 Models? An Empirical Study
- links: https://arxiv.org/pdf/2404.14047
- ideas:
  1. /Round-To-Nearest(RTN)/ rounding quantization method.
  2. /LORA/ find tuning quantization
     *What's the difference between Post-Training Quantization and LORA find-tuning?*

      

-------------
** 20240521
- paper: A Survey on Retrieval-Augmented Text Generation for Large Language Models
- links: https://arxiv.org/pdf/2404.10981
- ideas:
  1.  the /RAG paradigm/ into four categories: pre-retrieval, retrieval, post-retrieval, and generation



-------------
** 20240522
- paper: Best Practices and Lessons Learned on Synthetic Data for Language Models
- links: https://arxiv.org/pdf/2404.07503
- ideas:
  1.  Training with synthetic data makes evaluation decontamination harder.

      


-------------
** 20240523
- paper: Exploring the Limits of Language Modeling
- links: https://arxiv.org/pdf/1602.02410
- ideas:
  1. The goal of LM is to learn a probability distribution over sequences of symbols pertaining to a language



* 20240524: start to follow [[https://x.com/keshavchan/status/1787861946173186062][llya-30]], well-written Blog are considered as paper as well.

-------------
** 20240524
- paper: The First Law of Complexodynamics
- links: https://scottaaronson.blog/?p=762
- ideas:
  1. quote1: /why does “complexity” or “interestingness” of physical systems seem to increase with time and then hit a maximum and decrease, in contrast to the entropy, which of course increases monotonically?/
  *Question: What's the difference between entropy in physics and information theory?*
  2. suppose: /Kolmogorov complexity/ to define /entropy./
  3. quote2: First Law of Complexodynamics,” exhibiting exactly the behavior that Sean wants: small for the initial state, large for intermediate states, then small again once the mixing has finished.
  

--------
** 20240525
- paper: The Unreasonable Effectiveness of Recurrent Neural Networks
- links: https://arc.net/folder/D0472A20-9C20-4D3F-B145-D2865C0A9FEE
- ideas:
  1. quote1: If training vanilla neural nets is optimization over functions, training recurrent nets is optimization over programs.
    *Good Resources For Learning RNN*: 
      * [[https://towardsdatascience.com/recurrent-neural-networks-rnns-3f06d7653a85]]
      * [[https://github.com/karpathy/char-rnn]]


--------
** 20240526
- paper: Recurrent Models of Visual Attention
- links: https://arc.net/folder/D0472A20-9C20-4D3F-B145-D2865C0A9FEE
- ideas:
  1. quote1: /The model is a recurrent neural network (RNN) which processes inputs sequentially, attending to different locations within the images (or video frames) one at a time, and incrementally combines information from these fixations to build up a dynamic internal representation of the scene or environment./
  2.  Partially Observable Markov Decision Process (POMDP). 
--------

------
** 20240527
- paper: Neural Turing Machines
- links: https://arxiv.org/pdf/1410.5401
- ideas:
  1. quote1: /Fodor and Pylyshyn (Fodor and Pylyshyn, 1988) famously made two barbed claims about the limitations of neural networks for cognitive modeling. They first objected that connectionist theories were incapable of variable-binding, or the assignment of a particular datum to a particular slot in a data structure./


     #+begin_src txt
Neural Turing Machine:

                      External Input          External Output
                             \                  /
                              \                /
                             +------------+
                             | Controller |
                             +------------+
                               /      \
                              /        \
                       +-----------+   +-----------+
                       | Read Heads|   | Write Heads|
                       +-----------+   +------------+
      
#+end_src


------
** 20240528
- paper: Relational recurrent neural networks
- links: https://arxiv.org/pdf/1806.01822
- ideas:
  1. claim1: /Relational Memory Core (RMC)/ – which employs multi-head dot product attention to allow memories to
    interact

     #+begin_src txt

                        CORE

                    Prev. Memory
                         |
                         v
     +-------------------+------------------+
     |                   A                  |
     |               +----+----+            |
     |               |    |    |            |
     |               |  Residual            |
     |               +----+----+            |
     |                    |                 |
     |                    v                 |
     |                  +----+              |
     |                  | MLP |             |
     |                  +----+              |
     |                    |                 |
     |                Residual              |
     |                    |                 |
     +-------------------+------------------+
                         |
                         v
                       Output


         MULTI-HEAD DOT PRODUCT ATTENTION

          Memory
            |
            v
     +-------------------------+
     |    W_q   W_k   W_v      |
     |     |     |     |       |
     | query key value         |
     | (q1)  (k1)  (v1)        |
     |     \   |   /           |
     |      \  |  /            |
     |       softmax(QK^T)V    |
     |            |            |
     |            v            |
     |      Updated Memory     |
     +-------------------------+

Compute attention weights
Queries (Q)            Keys (K)               Weights
+---+---+---+         +---+---+---+         +---+---+---+
|q1 |q2 |...|         |k1 |k2 |...|         |w1 |w2 |...|
+---+---+---+         +---+---+---+         +---+---+---+

Normalize weights with row-wise softmax
Normalized Weights
+---+---+---+
| w1,1 w1,2...|
| w2,1 w2,2...|
| ...         |
+---+---+---+

Compute weighted average of values
Values (V)                Weighted Values
+---+---+---+         +---+---+---+
|v1 |v2 |...|         |wv1|wv2|...|
+---+---+---+         +---+---+---+

Return updated memory
Updated Memory
+---+---+---+
| M1 | M2 |...|
+---+---+---+
#+end_src



------
** 20240529
- paper: Keeping Neural Networks Simple by Minimizing the Description Length of the Weights
- links: https://www.cs.toronto.edu/~hinton/absps/colt93.pdf
- ideas:
  1. quote1: The Minimum Description Length Principle (Rissanen, 1986) asserts that the best model of some data is the one that minimizes the combined cost of describing the model and describing the misfit between the model and the data. 
