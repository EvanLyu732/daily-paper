#+TITLE: Evan's Daily Paper 
#+AUTHOR: Evan Lyu
#+auto_tangle: t
#+DESCRIPTION: Evan's Daily Paper
#+STARTUP: showeverything
#+STARTUP: align
#+OPTIONS: tocs:3

/I am *not* a researcher or scientist. Just a regular guy who follow my interest try to figure out some interesting stuff. Since I love emacs and org mode. Why not track all the papers that i read in a single org file? This is a lifelong repo./

* Content

| Date     | *Paper*                                                                                |
| [[20240423]] | A Survey of Embodied AI: From Simulators to Research Tasks                             |
| [[20240424]] | Why Functional Programming Matters                                                     |
| [[20240425]] | Recurrent Neural Networks (RNNs): A gentle Introduction and Overview                   |
| [[20240426]] | Neural Machine Translation by Jointly Learning to Align and Translate                  |
| [[20240427]] | A General Survey on Attention Mechanisms in Deep Learning                              |
| [[20240428]] | MEGALODON: Efficient LLM Pretraining and Inference with Unlimited Context Length       |
| [[20240429]] | Mega: Moving Average Equipped Gated Attention                                          |
| [[20240430]] | The Next Decade in AI                                                                  |
| [[20240501]] | The Bitter Lesson                                                                      |
| [[20240502]] | KAN: Kolmogorov–Arnold Networks                                                        |
| [[20240503]] | Multilayer feedforward networks are universal approximators                            |
| [[20240504]] | Sequence to Sequence Learning with Neural Networks                                     |
| [[20240505]] | Translating Videos to Natural Language Using Deep Recurrent Neural Networks            |
| [[20240506]] | Summarizing Source Code using a Neural Attention Model                                 |
| [[20240507]] | Learning to Control Fast-Weight Memories: An Alternative to Dynamic Recurrent Networks |
| [[20240508]] | Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention          |
| [[20240509]] | BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding       |
| [[20240510]] | Language Models are Unsupervised Multitask Learners                                    |
| [[20240511]] | Improving Language Understanding by Generative Pre-Training                            |
| [[20240512]] | Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond               |
| [[20240513]] | Cramming: Training a Language Model on a Single GPU in One Day                         |
| [[20240514]] | Autonomous LLM-driven research from data to human-verifiable research papers           |
| [[20240515]] | LoRA: Low-Rank Adaptation of Large Language Models                                     |
| [[20240516]] | When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively           |
| [[20240517]] | A PRIMER ON THE INNER WORKINGS OF TRANSFORMER-BASED LANGUAGE MODELS                    |
| [[20240518]] | Is artificial consciousness achievable? Lessons from the human brain                   |
| [[20240519]] | Teaching Algorithm Design: A Literature Review                                         |
| [[20240520]] | How Good Are Low-bit Quantized LLAMA3 Models? An Empirical Study                       |
| [[20240521]] | A Survey on Retrieval-Augmented Text Generation for Large Language Models              |
| [[20240522]] | Best Practices and Lessons Learned on Synthetic Data for Language Models               |
| [[20240523]] | Exploring the Limits of Language Modeling                                              |
| [[20240524]] | The First Law of Complexodynamics                                                      |
| [[20240525]] | The Unreasonable Effectiveness of Recurrent Neural Networks                            |
| [[20240526]] | Recurrent Models of Visual Attention                                                   |
| [[20240527]] | Neural Turing Machines                                                                 |
| [[20240528]] | Relational recurrent neural networks                                                   |
| [[20240529]] | Keeping Neural Networks Simple by Minimizing the Description Length of the Weights     |
| [[20240530]] | RECURRENT NEURAL NETWORK REGULARIZATION                                                |
| [[20240531]] | Layer Normalization                                                                    |
| [[20240601]] | Scaling Laws for Neural Language Models                                                |
| [[20240602]] | Deep Speech 2: End-to-End Speech Recognition in English and Mandarin                   |
| [[20240603]] | A Tutorial Introduction to the Minimum Description Length Principle                    |
| [[20240604]] | ORDER MATTERS: SEQUENCE TO SEQUENCE FOR SETS                                           |
| [[20240605]] | Pointer Networks                                                                       |
| [[20240606]] | Deep Residual Learning for Image Recognition                                           |
| [[20240607]] | The Shattered Gradients Problem: If resnets are the answer, then what is the question? |
| [[20240608]] | Scaling and evaluating sparse autoencoders                                             |
| [[20240612]] | Identity Mappings in Deep Residual Networks                                            |
| [[20240613]] | Quantifying the Rise and Fall of Complexity in Closed Systems: The Coffee Automaton    |
| [[20240614]] | VARIATIONAL LOSSY AUTOENCODER                                                          |
|          |                                                                                        |
|          |                                                                                        |
|          |                                                                                        |
|          |                                                                                        |


-----
** 20240423
- Paper: A Survey of Embodied AI: From Simulators to Research Tasks
- Links: https://arxiv.org/pdf/2103.04918.pdf
- Ideas:
  1. Embodied AI Simulators: DeepMind Lab, AI2-THOR, SAPIEN, VirtualHome, VRKitchen, ThreeDWorld, CHALET, iGibson, and Habitat-Sim.

-----
** 20240424
- Paper: Why Functional Programming Matters
- Links: https://www.cs.kent.ac.uk/people/staff/dat/miranda/whyfp90.pdf
- Ideas:
  1. functional programming can improve modularization in an maintainable way
     using high-order function and lazy evaluation 

-----
** 20240425
- Paper: Recurrent Neural Networks (RNNs): A gentle Introduction and Overview 
- Links: https://arxiv.org/pdf/1912.05911.pdf
- Ideas:
  1. RNN deal with /sequence/ data.
  2. BPTT (Back Propagation Through Time): store weight when processed through each loss term
  3. LSTM (Long Short-Term Memory): design to handle vanish graident problems and introduce the /gated cell/ to store more information (*What information*?)
  4. DRNN (Deep Recurrent Neural Networks): stack ordinary RNN together.
  5. BRNN (Bidirectional Recurrent Neural Networks): /the authors create the section, but i do not get any ideas./
  6. Seq2Seq: /What problems does seq2seq or encoder-decoder structure solves?/
  7. Attention & Transformers: /Why Attentions works?/ /Why Skip-Connection works?/
  8. Pointer Networks

-----
** 20240426
- Paper: Neural Machine Translation by Jointly Learning to Align and Translate
- Links: https://arxiv.org/pdf/1409.0473
- Ideas:
  *What is the difference with encoder-decoder architecture?*
  1. this link may helps https://slds-lmu.github.io/seminar_nlp_ss20/attention-and-self-attention-for-nlp.html
  2. bidirectional RNN as encoder and a decoder that search through a sources sentences during translation. The architecture *lead to*
     a *attention* mechansim in the decoder.

-----
** 20240427
- paper: A General Survey on Attention Mechanisms in Deep Learning
- links: https://arxiv.org/pdf/2203.14263
- ideas:
  1. authors define a /task model/, which contains /four component/, 1. /the feature model/ 2. /the query model/ 3. /the attention model/ 4. /the output model/
  2. /feature model/: used to extract features can be RNN or CNN and ...., for turning o$Xn$ into $fn$
  3. /query model/: a /query/ tell which feature $fn$ to attend to.
  4. /attention model/: given input query $qn$ and features vectors $fn$, the model extract the key matrix $Kn$ and value matrix $Vn$ from $fn$. Traditionaly, this process can be achived by linear transformation and use weight matrix $Wk$ and $Wv$.
  5. /attention mechanisms/ can be classify into three categories: query-related, feature-related and general(not relate to query or feature).

 *To learn more about attention mechanisms, this page https://slds-lmu.github.io/seminar_nlp_ss20/attention-and-self-attention-for-nlp.html and 3blue1brown video https://www.3blue1brown.com/lessons/attentionare are helpful*

------
** 20240428
- paper: MEGALODON: Efficient LLM Pretraining and Inference with Unlimited Context Length
- links: https://arxiv.org/pdf/2404.08801
- ideas:
  1. traditional transformer: computation complexity, limited inductive bias.
  2. introduce the /complex exponential moving average(CEMA)/ components, timestamp normalization layer, normalized attention and pre-norm with two-hop residual configuraion.

     *Q1: This paper is based on the architecture of MEGA, But What is MEGA*?
     *Q2: Why this architecture and deal with unlimited length?*
     #+begin_quote
Evaluation on long-context modeling, including
perplexity in various context lengths up to 2M and long-context QA tasks in Scrolls (Parisotto et al.,

#+end_quote
*not understand*...
     
------
** 20240429
- paper: Mega: Moving Average Equipped Gated Attention
- links: https://arxiv.org/pdf/2209.10655
- ideas:
   1. sequence modeling common approaches: self-attention and EMA(exponential moving average)
     *Well, this kind of theortical paper is too difficult for me, mayme i should start with some basic ideas and understand the concepts by doing project.*
     
------
** 20240430
- paper: The Next Decade in AI
- links: https://arxiv.org/pdf/2002.06177
- ideas: 
  1. authors cites "The Bitter Lesson" - By Rich Sutton, i have seen this paper in many places. I should check out this paper.
  2. claim1: =/to build a robust, knowledge-driven approach to AI we must have the machinery of symbol-manipulation in our toolkit. Too much of useful knowledge is abstract to make do without tools that represent and manipulate abstraction, and to date, the only machinery that we know of that can manipulate such abstract knowledge reliably is the apparatus of symbol-manipulation/.=
  3. claim2: robust artificial intelligences properties:
     * have the ability to learn new knowledge
     * can learn knowledged that is symbolically represented.
     * significant knowledge is likely to be abstract.
     * rules and exceptions are co-existed
     * Some significant fraction of the knowledge that a robust system is likely to be causal, and to support counterfactuals.
     * Some small but important subset of human knowledge is likely to be innate; robust AI, too, should start with some important prior knowledge.
  4. claim3: rather than starting each new AI system from scratch, as a blank slate, with little knowledge of the world, we should seek to build learning systems that start with initial frameworks for domains like time, space, and causality, in order to speed up learning and massively constrain the hypothesis space.
  5. knowledge by itself it not enough. knowledge put into practice with tool of reasoning.
     #+begin_quote
a reasoning system that can leverage large-scale background knowledge
efficiently, even when available information is incomplete is a prerequisite to robustness.
#+end_quote

------
** 20240502
- paper: KAN: Kolmogorov–Arnold Networks
- links: https://arxiv.org/pdf/2404.19756
- ideas
  1. claim1: Kolmogorov-Arnold representation theorem
     *What is Kolmogorov-Arnold representation theorem? Why it can represented any function like Universal Approximation Theorem?* 
  2. claim2: MLP: learnable weights on edges, KAN learnable activation functions on edges.
     *TOMORRORW PAPER IS ABOUT UNIVERSAL APPROXIMATION THEOREM*
  3. claim3:  KANs’ nodes simply sum incoming signals without applying any non-linearities
  4. claim4:  KANs are nothing more than combinations of splines
     *What is splines?*
  5. claim5: Currently, the biggest bottleneck of KANs lies in its slow training. KANs are usually 10x slower than MLPs, given the same number of parameters. We should be honest that we did not try hard to optimize KANs’ efficiency though, so we deem KANs’ slow training more as an engineering problem to be improved in the future rather than a fundamental limitation. If one wants to train a model fast, one should use MLPs. In other cases, however, KANs should be comparable or better than MLPs, which makes them worth trying.  
------
------
** 20240503
- paper: Multilayer feedforward networks are universal approximators 
- links: https://cognitivemedium.com/magic_paper/assets/Hornik.pdf 
- ideas:
  1. claim1: Advocates of the virtues of multilayer feedfor- ward networks (e.g., Hecht-Nielsen, 1987) often cite /Kolmogorov’s/ (1957) superposition theorem or its more recent improvements (e.g.. Lorentz, 1976) in support of their capabilities. However, these results require a different unknown transformation (g in Lorentz’s notation) for each continuous function to be represented, while specifying an exact upper limit to the number of intermediate units needed for the representation.
  2. Anyway, this paper prove multilayer feedforward networks is a class of universal approximators.
     *While reading this paper, i am wondering why encoder-decoder structure network work? who proposed that? This is tomorrow topic.*


------
** 20240504
- paper: Sequence to Sequence Learning with Neural Networks 
- links: https://arxiv.org/pdf/1409.3215
- ideas:
  1. claim1:  DNNs can only be applied to problems whose inputs and targets can be sensibly encoded with vectors of fixed dimensionality.
  2. claim2:  network architecture,  one LSTM for encoder and another LSTM for decoder.
     *What is the encoder and the decoder has different network structure?*
------

** 20240505
- paper: Translating Videos to Natural Language Using Deep Recurrent Neural Networks 
- links: https://arxiv.org/pdf/1412.4729
- ideas:
  1. claim1: 
     ```
     video -> cnn -> lstm -> label
     ```

     *It seems like features extraction network is a kind of encoder-decoder structure networks.*
------

** 20240506
- paper: Summarizing Source Code using a Neural Attention Model 
- links: https://github.com/sriniiyer/codenn/blob/master/summarizing_source_code.pdf 
- ideas:
  1. claim1:
       dataset: stackoverflow that contains c# tag
       model: LSTM 

  *Today paper is about llm in code generation, i chose this paper from this slides https://webstanford.edu/class/cs224g/slides/Code%20Generation%20with%20LLMs.pdf and i discover ==Standford CS 224G=.= Great Resources for keeping track to frontier llm application.*


** 20240507
- paper: Learning to Control Fast-Weight Memories: An Alternative to Dynamic Recurrent Networks
- links: https://ieeexplore.ieee.org/document/6796337
- ideas:
  1. two feedword networks. first network produce "fast-weight" as short-term memory, memory controller 
     *Well, This URL is worth a look. https://people.idsia.ch//~juergen/most-cited-neural-nets.html*


** 20240508
- paper: Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention 
- links: https://arxiv.org/pdf/2006.16236
- ideas:
  1. claim1: tranditional transformers require quadratics memories, such as for $N$ input. The time complexity is $O(N_2)$. This paper propose linear transformation.
     *What's the difference between attention layer and self-attention layer?*
     #+begin_quote
every transformer can be seen as a recurrent neural network
#+end_quote




** 20240509
- paper: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
- links: https://arxiv.org/pdf/1810.04805 
- ideas:
  1. claim1: two methods for apply pre-trained language models to downstream tasks( feature-based and find-tuning)
     


** 20240510
- paper: Language Models are Unsupervised Multitask Learners
- links: https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf
- ideas:
  1. dataset WebText (millions of webpage)
  2. language modeling (unsupervised distribution estimation of examples, each example contains a length of symbols.)
        https://github.com/codelucas/newspaper
     


** 20240511
- paper: Improving Language Understanding by Generative Pre-Training
- links: https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf
- ideas:
  1. By pre-training on a diverse corpus with long stretches of contiguous text our model acquires significant world knowledge and ability to process long-range dependencies which are then successfully transferred to solving discriminative tasks such as question answering, semantic similarity assessment, entailmentdetermination, and text classification, improving the state of the art on 9 of the 12 datasets we study.

     *What sources of corpus is suitable for pre training?*



** 20240512
- paper: Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond
- links: https://arxiv.org/pdf/2304.13712
- ideas:
  *What's the difference between encoder-decoder structure with decoder only model?*
  1. decoder-only model
  2. nlu task: text classification, named entity recognition (NER),
entailment prediction, and so on. 
  3. nlg task: Natural Language Generation broadly encompasses two major categories of tasks, with the goal of creating coherent, meaningful, and contextually appropriate sequences of symbols. 
 

-------------
** 20240513
- paper: Cramming: Training a Language Model on a Single GPU in One Day
- links: https://arxiv.org/pdf/2212.14034
- ideas:
  no ideas here, i follow this https://magazine.sebastianraschka.com/p/understanding-large-language-models to read paper. But maybe one day this paper would be a good start for implementing BERT in consumer compute. I'm not sure if i will do this experiment. 


-------------
** 20240514
- paper: Autonomous LLM-driven research from data to human-verifiable research papers
- links: https://arxiv.org/pdf/2404.17605
- ideas:
  1. this paper propose data to paper. crazy idea...
  2. 
#+begin_quote
Starting with a human-provided dataset.the process is designed to raise hypotheses, write, debug and execute code to analyze the
data and perform statistical tests, interpret the results and write well-structured scientific
papers which not only describe results and conclusions but also transparently delineate the
research methodologies, allowing human scientists to understand, repeat and verify the
analysis. The discussion on emerging guidelines for AI-driven science (22) have served as a
design framework for data-to-paper, yielding a fully transparent, traceable and verifiable
workflow, and algorithmic \u201cchaining\u201d of data, methodology and result allowing to trace
downstream results back to the part of code which generated them. The system can run with
or without a predefined research goal (fixed/open-goal modalities) and with or without human
interactions and feedback (copilot/autopilot modes). We performed two open-goal and two
fixed-goal case studies on different public datasets (24\u201327) and evaluated the AI-driven
research process as well as the novelty and accuracy of created scientific papers. We show
that, running fully autonomously (autopilot), data-to-paper can perform complete and correct
run cycles for simple goals, while for complex goals, human co-piloting becomes critical.  
#+end_quote

-------------
** 20240515
- paper: LoRA: Low-Rank Adaptation of Large Language Models
- links: https://arxiv.org/pdf/2106.09685
- ideas:
  1. claim1: LoRA: which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable pa-rameters for downstream tasks


-------------
** 20240516
- paper: When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively
- links: https://arxiv.org/pdf/2404.19705
- ideas:
  1. claim1: this paper propose a method that when LLM generate token by <RET>, it use ir system for retrieve outer sources.


-------------
** 20240517
- paper: A PRIMER ON THE INNER WORKINGS OF TRANSFORMER-BASED LANGUAGE MODELS
- links: https://arxiv.org/pdf/2405.00208
- ideas:
  1. claim1: layer normalization is a common operation used to stabilize the training process of deep neural networks
     *This paper is too long for me to digest. Maybe one day i'll come to visit when i project transfromer architecture*

-------------
** 20240518
- paper: Is artificial consciousness achievable? Lessons from the
human brain
- links: https://arxiv.org/pdf/2405.04540
- ideas:
  1. claim1:
     #+begin_quote
Given this uncertainty, we recommend not to use the same general term (i.e., consciousness) for both humans and artificial systems; to clearly specify the key differences between them; and, last but not least, to be very clear about which dimension and level of consciousness the artificial system may possibly be capable of displaying.
#+end_quote

-------------
** 20240519
- paper: Teaching Algorithm Design: A Literature Review
- links: https://arxiv.org/pdf/2405.00832
- ideas:
  1. claim: *Systematic literature reviews*
     * Research Question
     * Protocol Development
     * Search Databases
     * Screen Studies
     * Extract Data
     * Assess Quality
     * Synthesize Data
     * Report Findings


-------------
** 20240520
- paper: How Good Are Low-bit Quantized LLAMA3 Models? An Empirical Study
- links: https://arxiv.org/pdf/2404.14047
- ideas:
  1. /Round-To-Nearest(RTN)/ rounding quantization method.
  2. /LORA/ find tuning quantization
     *What's the difference between Post-Training Quantization and LORA find-tuning?*

      

-------------
** 20240521
- paper: A Survey on Retrieval-Augmented Text Generation for Large Language Models
- links: https://arxiv.org/pdf/2404.10981
- ideas:
  1.  the /RAG paradigm/ into four categories: pre-retrieval, retrieval, post-retrieval, and generation



-------------
** 20240522
- paper: Best Practices and Lessons Learned on Synthetic Data for Language Models
- links: https://arxiv.org/pdf/2404.07503
- ideas:
  1.  Training with synthetic data makes evaluation decontamination harder.

      


-------------
** 20240523
- paper: Exploring the Limits of Language Modeling
- links: https://arxiv.org/pdf/1602.02410
- ideas:
  1. The goal of LM is to learn a probability distribution over sequences of symbols pertaining to a language



* 20240524: start to follow [[https://x.com/keshavchan/status/1787861946173186062][llya-30]], well-written Blog are considered as paper as well.

-------------
** 20240524
- paper: The First Law of Complexodynamics
- links: https://scottaaronson.blog/?p=762
- ideas:
  1. quote1: /why does “complexity” or “interestingness” of physical systems seem to increase with time and then hit a maximum and decrease, in contrast to the entropy, which of course increases monotonically?/
  *Question: What's the difference between entropy in physics and information theory?*
  2. suppose: /Kolmogorov complexity/ to define /entropy./
  3. quote2: First Law of Complexodynamics,” exhibiting exactly the behavior that Sean wants: small for the initial state, large for intermediate states, then small again once the mixing has finished.
  

--------
** 20240525
- paper: The Unreasonable Effectiveness of Recurrent Neural Networks
- links: https://arc.net/folder/D0472A20-9C20-4D3F-B145-D2865C0A9FEE
- ideas:
  1. quote1: If training vanilla neural nets is optimization over functions, training recurrent nets is optimization over programs.
    *Good Resources For Learning RNN*: 
      * [[https://towardsdatascience.com/recurrent-neural-networks-rnns-3f06d7653a85]]
      * [[https://github.com/karpathy/char-rnn]]


--------
** 20240526
- paper: Recurrent Models of Visual Attention
- links: https://arc.net/folder/D0472A20-9C20-4D3F-B145-D2865C0A9FEE
- ideas:
  1. quote1: /The model is a recurrent neural network (RNN) which processes inputs sequentially, attending to different locations within the images (or video frames) one at a time, and incrementally combines information from these fixations to build up a dynamic internal representation of the scene or environment./
  2.  Partially Observable Markov Decision Process (POMDP). 
--------

------
** 20240527
- paper: Neural Turing Machines
- links: https://arxiv.org/pdf/1410.5401
- ideas:
  1. quote1: /Fodor and Pylyshyn (Fodor and Pylyshyn, 1988) famously made two barbed claims about the limitations of neural networks for cognitive modeling. They first objected that connectionist theories were incapable of variable-binding, or the assignment of a particular datum to a particular slot in a data structure./


     #+begin_src txt
Neural Turing Machine:

                      External Input          External Output
                             \                  /
                              \                /
                             +------------+
                             | Controller |
                             +------------+
                               /      \
                              /        \
                       +-----------+   +-----------+
                       | Read Heads|   | Write Heads|
                       +-----------+   +------------+
      
#+end_src


------
** 20240528
- paper: Relational recurrent neural networks
- links: https://arxiv.org/pdf/1806.01822
- ideas:
  1. claim1: /Relational Memory Core (RMC)/ – which employs multi-head dot product attention to allow memories to
    interact

     #+begin_src txt

                        CORE

                    Prev. Memory
                         |
                         v
     +-------------------+------------------+
     |                   A                  |
     |               +----+----+            |
     |               |    |    |            |
     |               |  Residual            |
     |               +----+----+            |
     |                    |                 |
     |                    v                 |
     |                  +----+              |
     |                  | MLP |             |
     |                  +----+              |
     |                    |                 |
     |                Residual              |
     |                    |                 |
     +-------------------+------------------+
                         |
                         v
                       Output


         MULTI-HEAD DOT PRODUCT ATTENTION

          Memory
            |
            v
     +-------------------------+
     |    W_q   W_k   W_v      |
     |     |     |     |       |
     | query key value         |
     | (q1)  (k1)  (v1)        |
     |     \   |   /           |
     |      \  |  /            |
     |       softmax(QK^T)V    |
     |            |            |
     |            v            |
     |      Updated Memory     |
     +-------------------------+

Compute attention weights
Queries (Q)            Keys (K)               Weights
+---+---+---+         +---+---+---+         +---+---+---+
|q1 |q2 |...|         |k1 |k2 |...|         |w1 |w2 |...|
+---+---+---+         +---+---+---+         +---+---+---+

Normalize weights with row-wise softmax
Normalized Weights
+---+---+---+
| w1,1 w1,2...|
| w2,1 w2,2...|
| ...         |
+---+---+---+

Compute weighted average of values
Values (V)                Weighted Values
+---+---+---+         +---+---+---+
|v1 |v2 |...|         |wv1|wv2|...|
+---+---+---+         +---+---+---+

Return updated memory
Updated Memory
+---+---+---+
| M1 | M2 |...|
+---+---+---+
#+end_src



------
** 20240529
- paper: Keeping Neural Networks Simple by Minimizing the Description Length of the Weights
- links: https://www.cs.toronto.edu/~hinton/absps/colt93.pdf
- ideas:
  1. quote1: The Minimum Description Length Principle (Rissanen, 1986) asserts that the best model of some data is the one that minimizes the combined cost of describing the model and describing the misfit between the model and the data. 


------
** 20240530
- paper: RECURRENT NEURAL NETWORK REGULARIZATION
- links: https://arxiv.org/pdf/1409.2329
- ideas:
  1. claim1:  The main contribution of this paper is a recipe for applying dropout to LSTMs in a way that successfully reduces overfitting


------
** 20240531
- paper: Layer Normalization
- links: https://arxiv.org/pdf/1607.06450
- ideas:
  1. claim1:  batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case.
  2. claim2:  layer normalization, a simple normalization method to improve the training speed for various neural network models. Unlike batch normalization, the proposed method directly
     estimates the normalization statistics from the summed inputs to the neurons within a hidden layer so the normalization does not introduce any new dependencies between training cases
  3. claim3:  Layer normalization does not have such problem because its normalization terms depend only on the summed inputs to a layer at the current time-step.

     *Well, here is a question /what is Internal Covariate Shift/, /why Internal Covariate Shift becomes a problem/ ?*

     Answer by kimi:
     #+begin_quote

Imagine you're playing a game where you have to sort different colored balls into different baskets. Each basket is for a different color, like a basket for red balls, a basket for blue balls, and so on.

Now, suppose at the start you only have a few balls, and they're not evenly spread out in terms of color. Some baskets have more red balls, some have more blue balls. When you start sorting, you learn how to tell the colors apart based on these limited balls. But if the rules of your game suddenly change, like if you get a lot more balls, or the colors of the balls change, then the way you learned to sort them might not be so accurate anymore.

In machine learning, "Internal Covariate Shift" is a bit like that situation. When we train a machine learning model, we usually use a lot of data to teach it. *But if we change the distribution of the data during training, or if we don't have enough data to represent all the possible situations, then what the model learned might change, too. That's what we call "Internal Covariate Shift."*

Just like how the way you learned to sort the balls at the start of the game might not be accurate if the game's rules change, the machine learning model might need to adjust if the data distribution changes to keep being accurate.

#+end_quote

------
** 20240601
- paper: Scaling Laws for Neural Language Models
- links: https://arxiv.org/pdf/2001.08361
- ideas:
  1. claim1:  Larger models require /fewer samples/ to reach the same performance
     * Performance depends strongly on scale, weakly on model shape
       #+begin_quote
Simple equations govern the dependence of overfitting on model/dataset size and the
dependence of training speed on model size. These relationships allow us to determine the
optimal allocation of a fixed compute budget.
#+end_quote

------
** 20240602
- paper: Deep Speech 2: End-to-End Speech Recognition in English and Mandarin
- links: https://arxiv.org/pdf/1512.02595
- ideas:
  1. claim1:  
     #+begin_quote
To achieve these results, we have explored various network architectures, finding several effective
techniques: enhancements to numerical optimization through SortaGrad and Batch Normalization,
evaluation of RNNs with larger strides with bigram outputs for English, searching through both
bidirectional and unidirectional models. This exploration was powered by a well optimized, High
Performance Computing inspired training system that allows us to train new, full-scale models on
our large datasets in just a few days.
#+end_quote

*This paper focuses more on the engineering aspects of the topic.*


------
** 20240603
- paper: A Tutorial Introduction to the Minimum Description Length Principle
- links: https://arxiv.org/pdf/math/0406077
- ideas:
  1. claim1:  we can therefore say that the more we are able to compress the data, the more we have learned about the data.
  2. claim2:  The Fundamental Idea: Learning as Data Compression
  3. claim3:
     #+begin_quote
To formalize our ideas, we need to decide on a description method, that is, a formal
language in which to express properties of the data. The most general choice is a
general-purpose2 computer language such as C or Pascal. This choice leads to the
definition of the Kolmogorov Complexity [Li and Vit´anyi 1997] of a sequence as the
length of the shortest program that prints the sequence and then halts. The lower
the Kolmogorov complexity of a sequence, the more regular it is. 
#+end_quote

#+begin_quote
However, it turns
out that for every two general-purpose programming languages A and B and every
data sequence D, the length of the shortest program for D written in language A and
the length of the shortest program for D written in language B differ by no more
than a constant c, which does not depend on the length of D. This so-called invari-
ance theorem says that, as long as the sequence D is long enough, it is not essential
which computer language one chooses, as long as it is general-purpose.

#+end_quote

#+begin_quote
*MDL: The Basic Idea*
The goal of statistical inference may be cast as trying to /find regularity in the data/.
‘Regularity’ may be identified with ‘/ability to compress/’. MDL combines these two
insights by viewing learning as data compression: it tells us that, for a given set of
hypotheses /H/ and data set /D/, we should try to find the hypothesis or combination
of hypotheses in /H/ that compresses /D/ most.
#+end_quote
.... (have not finished yet.)

*This book delves into the fundamental building blocks of current deep learning systems, but it requires a solid background in information theory to fully grasp the underlying concepts.*


------
** 20240605
- paper: pointer networks
- links: https://arxiv.org/pdf/1506.03134
- ideas:
  1. claim1: Our model solves the problem of variable size output dictionaries using a recently proposed mechanism of neural attention. It differs from the previous attention attempts in
that, instead of using attention to blend hidden units of an encoder to a context vector at each decoder step, it uses attention as a pointer to select a member of the input sequence as the output


------
** 20240606
- paper: Deep Residual Learning for Image Recognition
- links: https://arxiv.org/pdf/1512.03385
- ideas:
  1. claim1: Deep networks naturally integrate low/mid/highlevel features
  2. claim2: When deeper networks are able to start converging, a degradation problem has been exposed: with the network depth increasing, accuracy gets saturated (which might be
unsurprising and then degrades rapidly. Unexpectedly, such degradation is not caused by overfitting, and adding more layers to a suitably deep model leads to higher training error.
  *What pr

------
** 20240607
- paper: The Shattered Gradients Problem: If resnets are the answer, then what is the question?
- links: https://arxiv.org/pdf/1702.08591 
- ideas:
  1. claim1: If resnets are the solution, then what is the problem?
  2. claim2: a previously unnoticed difficulty with gradients in deep rectifier networks that is orthogonal to vanishing and exploding gradients. The shattering gradients problem
is that, as depth increases, gradients in standard feedforward networks increasingly resemble white noise.
  1. claim3: The shattered gradient problem is that the spatial structure of gradients is progressively obliterated as neural nets deepen.
  2. claim4: Introducing skip-connections allows much deeper networks to be trained (Srivastava et al., 2015; He et al., 2016b;a; Greff et al., 2017). Skip-connections signif- icantly change the correlation structure of gradients
  3. claim5: Batch normalization was introduced to reduce covariate shift (Ioffe & Szegedy, 2015). However, it has other effects that are less well-known – and directly impact the correlation structure of gradients.
     *Maybe to really understand reset-net and shattered reqiure coding something.*

------
** 20240608
*OpenAI new paper about using top-k sparse autoencoder for neural network explaination*
- paper: Scaling and evaluating sparse autoencoders
- links:https://cdn.openai.com/papers/sparse-autoencoders.pdf 
- ideas:
  1. I write a chinese blog about this paper, check this [[https://evanlyu732.github.io/blog10/][link]]


------
** 20240612
*Continue llya-30* 
- paper: Identity Mappings in Deep Residual Networks
- links: https://arxiv.org/pdf/1603.05027
- ideas:
  1. claim1: This paper investigates the propagation formulations behind the connection
mechanisms of deep residual networks. Our derivations imply that identity short-
cut connections and identity after-addition activation are essential for making
information propagation smooth.

------
** 20240613
- paper: Quantifying the Rise and Fall of Complexity in Closed Systems: The Coffee Automaton
- links: https://arxiv.org/pdf/1405.6903
- ideas:
  1. claim1: Just as we can reason about the disorder of the coffee cup system, we can also consider its
“complexity.” Informally, by complexity we mean the /amount of information/ needed to describe
everything “interesting” about the system. 

*I don't know why paper have relation to deep learning, so i ask GPT4 and here is the answers may help you:*

#+begin_quote
The paper "Quantifying the Rise and Fall of Complexity in Closed Systems: The Coffee Automaton" relates to deep learning in several ways:

1. **Optimization and Convergence**: The process of complexity rising and falling in a closed system is similar to the optimization and convergence of neural networks during training. As a model learns, its complexity increases, reaches a peak, and then stabilizes as it approaches an optimal solution.

2. **Dynamic Systems**: Both the Coffee Automaton and deep learning models are dynamic systems that evolve over time. Understanding how complexity changes in these systems can provide insights into the behavior and stability of neural networks.

3. **Pattern Recognition**: The study of how patterns emerge and disappear in the automaton parallels how deep learning models recognize and simplify patterns in data. This understanding can help improve model design and efficiency.

4. **Entropy and Information Theory**: The concepts of entropy and information theory used to quantify complexity in the paper are also fundamental to understanding the information processing capabilities of deep learning models.

These parallels highlight the broader applicability of principles from the study of physical systems to the field of deep learning, providing valuable insights into the dynamics and optimization of neural networks.
#+end_quote


*I spend some time on learning how to read paper, so I upgrade my method from now.*
------
 *Core principle*:

 - no quote from text.
 - always ask question.
 - for every paper use at least one-two sentences to summary paper idea.
 - no missing math formula 

** 20240614
- paper: VARIATIONAL LOSSY AUTOENCODER
- links: https://arxiv.org/pdf/1603.05027

*here i try to strcture my reading process.*

    * first-pass:

        Q: what is paper about? 
        A: this paper proposed a method that combining VAE(Variational Autoencoder) with neural autoregressive models,
           which increase the flexiability of global latent codes for various problem and increase sparsity so that result can be better explaination and faster computing.

        Q: how does it improved compare to other works?
        A: to answer this question is hard. Becase i am limited in my understanding about different noun. but there is an excellent quote in this paper answer the question.
        #+begin_quote
However, earlier attempts at combining these two kinds of models have run into the problem that the autoregressive part of the model ends up explaining all structure in the data, while the latent variables are not used.
    #+end_quote

           
        Q: what is the main method in this work?
        A:
          * goal: given data, model auto learn features without interacting
          * method: 
              - VAE + autoregressive model (/but why?/)
              - 

        *Does density estimator and representation learning are different tasks?*

  ** My quesiton where does maximum likelihood comes from ? **

   * second pass:

        Q: 
        A: 

        Q: 
        A: 

        Q: 
        A: 
