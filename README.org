#+TITLE: Evan's Daily Paper 
#+AUTHOR: Evan Lyu
#+auto_tangle: t
#+DESCRIPTION: Evan's Daily Paper
#+STARTUP: showeverything
#+STARTUP: align
#+OPTIONS: tocs:3

/I am *not* a researcher or scientist. Just a regular guy who follow my interest try to figure out some interesting stuff. Since I love emacs and org mode. Why not track all the papers that i read in a single org file? This is a lifelong repo./

* Content

| Date     | *Paper*                                                                          |
| [[20240423]] | A Survey of Embodied AI: From Simulators to Research Tasks                       |
| [[20240424]] | Why Functional Programming Matters                                               |
| [[20240425]] | Recurrent Neural Networks (RNNs): A gentle Introduction and Overview             |
| [[20240426]] | Neural Machine Translation by Jointly Learning to Align and Translate            |
| [[20240427]] | A General Survey on Attention Mechanisms in Deep Learning                        |
| [[20240428]] | MEGALODON: Efficient LLM Pretraining and Inference with Unlimited Context Length |
| [[20240429]] | Mega: Moving Average Equipped Gated Attention                                    |


-----
** 20240423
- Paper: A Survey of Embodied AI: From Simulators to Research Tasks
- Links: https://arxiv.org/pdf/2103.04918.pdf
- Ideas:
  1. Embodied AI Simulators: DeepMind Lab, AI2-THOR, SAPIEN, VirtualHome, VRKitchen, ThreeDWorld, CHALET, iGibson, and Habitat-Sim.

-----
** 20240424
- Paper: Why Functional Programming Matters
- Links: https://www.cs.kent.ac.uk/people/staff/dat/miranda/whyfp90.pdf
- Ideas:
  1. functional programming can improve modularization in an maintainable way
     using high-order function and lazy evaluation 

-----
** 20240425
- Paper: Recurrent Neural Networks (RNNs): A gentle Introduction and Overview 
- Links: https://arxiv.org/pdf/1912.05911.pdf
- Ideas:
  1. RNN deal with /sequence/ data.
  2. BPTT (Back Propagation Through Time): store weight when processed through each loss term
  3. LSTM (Long Short-Term Memory): design to handle vanish graident problems and introduce the /gated cell/ to store more information (*What information*?)
  4. DRNN (Deep Recurrent Neural Networks): stack ordinary RNN together.
  5. BRNN (Bidirectional Recurrent Neural Networks): /the authors create the section, but i do not get any ideas./
  6. Seq2Seq: /What problems does seq2seq or encoder-decoder structure solves?/
  7. Attention & Transformers: /Why Attentions works?/ /Why Skip-Connection works?/
  8. Pointer Networks

-----
** 20240426
- Paper: Neural Machine Translation by Jointly Learning to Align and Translate
- Links: https://arxiv.org/pdf/1409.0473
- Ideas:
  *What is the difference with encoder-decoder architecture?*
  1. this link may helps https://slds-lmu.github.io/seminar_nlp_ss20/attention-and-self-attention-for-nlp.html
  2. bidirectional RNN as encoder and a decoder that search through a sources sentences during translation. The architecture *lead to*
     a *attention* mechansim in the decoder.

-----
** 20240427
- paper: A General Survey on Attention Mechanisms in Deep Learning
- links: https://arxiv.org/pdf/2203.14263
- ideas:
  1. authors define a /task model/, which contains /four component/, 1. /the feature model/ 2. /the query model/ 3. /the attention model/ 4. /the output model/
  2. /feature model/: used to extract features can be RNN or CNN and ...., for turning o$Xn$ into $fn$
  3. /query model/: a /query/ tell which feature $fn$ to attend to.
  4. /attention model/: given input query $qn$ and features vectors $fn$, the model extract the key matrix $Kn$ and value matrix $Vn$ from $fn$. Traditionaly, this process can be achived by linear transformation and use weight matrix $Wk$ and $Wv$.
  5. /attention mechanisms/ can be classify into three categories: query-related, feature-related and general(not relate to query or feature).

 *To learn more about attention mechanisms, this page https://slds-lmu.github.io/seminar_nlp_ss20/attention-and-self-attention-for-nlp.html and 3blue1brown video https://www.3blue1brown.com/lessons/attentionare are helpful*

------
** 20240428
- paper: MEGALODON: Efficient LLM Pretraining and Inference with Unlimited Context Length
- links: https://arxiv.org/pdf/2404.08801
- ideas:
  1. traditional transformer: computation complexity, limited inductive bias.
  2. introduce the /complex exponential moving average(CEMA)/ components, timestamp normalization layer, normalized attention and pre-norm with two-hop residual configuraion.

     *Q1: This paper is based on the architecture of MEGA, But What is MEGA*?
     *Q2: Why this architecture and deal with unlimited length?*
     #+begin_quote
Evaluation on long-context modeling, including
perplexity in various context lengths up to 2M and long-context QA tasks in Scrolls (Parisotto et al.,
2020) prove MEGALODONâ€™s ability to model sequences of unlimited length.
#+end_quote
*not understand*...
     
------
** 20240429
- paper: Mega: Moving Average Equipped Gated Attention
- links: https://arxiv.org/pdf/2209.10655
- ideas:
   1. sequence modeling common approaches: self-attention and EMA(exponential moving average)
     *Well, this kind of theortical paper is too difficult for me, mayme i should start with some basic ideas and understand the concepts by doing project.*
     
