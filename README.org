#+TITLE: Evan's Daily Paper 
#+AUTHOR: Evan Lyu
#+auto_tangle: t
#+DESCRIPTION: Evan's Daily Paper
#+STARTUP: showeverything
#+STARTUP: align
#+OPTIONS: tocs:3

/I am *not* a researcher or scientist. Just a regular guy who follow my interest try to figure out some interesting stuff. Since I love emacs and org mode. Why not track all the papers that i read in a single org file? This is a lifelong repo./

* Content

| Date     | *Paper*                                                                                |
| [[20240423]] | A Survey of Embodied AI: From Simulators to Research Tasks                             |
| [[20240424]] | Why Functional Programming Matters                                                     |
| [[20240425]] | Recurrent Neural Networks (RNNs): A gentle Introduction and Overview                   |
| [[20240426]] | Neural Machine Translation by Jointly Learning to Align and Translate                  |
| [[20240427]] | A General Survey on Attention Mechanisms in Deep Learning                              |
| [[20240428]] | MEGALODON: Efficient LLM Pretraining and Inference with Unlimited Context Length       |
| [[20240429]] | Mega: Moving Average Equipped Gated Attention                                          |
| [[20240430]] | The Next Decade in AI                                                                  |
| [[20240501]] | The Bitter Lesson                                                                      |
| [[20240502]] | KAN: Kolmogorov–Arnold Networks                                                        |
| [[20240503]] | Multilayer feedforward networks are universal approximators                            |
| [[20240504]] | Sequence to Sequence Learning with Neural Networks                                     |
| [[20240505]] | Translating Videos to Natural Language Using Deep Recurrent Neural Networks            |
| [[20240506]] | Summarizing Source Code using a Neural Attention Model                                 |
| [[20240507]] | Learning to Control Fast-Weight Memories: An Alternative to Dynamic Recurrent Networks |
| [[20240508]] | Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention          |
| [[20240509]] | BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding       |
| [[20240510]] | Language Models are Unsupervised Multitask Learners                                    |
| [[20240511]] | Improving Language Understanding by Generative Pre-Training                            |
| [[20240512]] | Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond               |
| [[20240513]] | Cramming: Training a Language Model on a Single GPU in One Day                         |
| [[20240514]] | Autonomous LLM-driven research from data to human-verifiable research papers           |
| [[20240515]] | LoRA: Low-Rank Adaptation of Large Language Models                                     |
|          |                                                                                        |
|          |                                                                                        |
|          |                                                                                        |
|          |                                                                                        |


-----
** 20240423
- Paper: A Survey of Embodied AI: From Simulators to Research Tasks
- Links: https://arxiv.org/pdf/2103.04918.pdf
- Ideas:
  1. Embodied AI Simulators: DeepMind Lab, AI2-THOR, SAPIEN, VirtualHome, VRKitchen, ThreeDWorld, CHALET, iGibson, and Habitat-Sim.

-----
** 20240424
- Paper: Why Functional Programming Matters
- Links: https://www.cs.kent.ac.uk/people/staff/dat/miranda/whyfp90.pdf
- Ideas:
  1. functional programming can improve modularization in an maintainable way
     using high-order function and lazy evaluation 

-----
** 20240425
- Paper: Recurrent Neural Networks (RNNs): A gentle Introduction and Overview 
- Links: https://arxiv.org/pdf/1912.05911.pdf
- Ideas:
  1. RNN deal with /sequence/ data.
  2. BPTT (Back Propagation Through Time): store weight when processed through each loss term
  3. LSTM (Long Short-Term Memory): design to handle vanish graident problems and introduce the /gated cell/ to store more information (*What information*?)
  4. DRNN (Deep Recurrent Neural Networks): stack ordinary RNN together.
  5. BRNN (Bidirectional Recurrent Neural Networks): /the authors create the section, but i do not get any ideas./
  6. Seq2Seq: /What problems does seq2seq or encoder-decoder structure solves?/
  7. Attention & Transformers: /Why Attentions works?/ /Why Skip-Connection works?/
  8. Pointer Networks

-----
** 20240426
- Paper: Neural Machine Translation by Jointly Learning to Align and Translate
- Links: https://arxiv.org/pdf/1409.0473
- Ideas:
  *What is the difference with encoder-decoder architecture?*
  1. this link may helps https://slds-lmu.github.io/seminar_nlp_ss20/attention-and-self-attention-for-nlp.html
  2. bidirectional RNN as encoder and a decoder that search through a sources sentences during translation. The architecture *lead to*
     a *attention* mechansim in the decoder.

-----
** 20240427
- paper: A General Survey on Attention Mechanisms in Deep Learning
- links: https://arxiv.org/pdf/2203.14263
- ideas:
  1. authors define a /task model/, which contains /four component/, 1. /the feature model/ 2. /the query model/ 3. /the attention model/ 4. /the output model/
  2. /feature model/: used to extract features can be RNN or CNN and ...., for turning o$Xn$ into $fn$
  3. /query model/: a /query/ tell which feature $fn$ to attend to.
  4. /attention model/: given input query $qn$ and features vectors $fn$, the model extract the key matrix $Kn$ and value matrix $Vn$ from $fn$. Traditionaly, this process can be achived by linear transformation and use weight matrix $Wk$ and $Wv$.
  5. /attention mechanisms/ can be classify into three categories: query-related, feature-related and general(not relate to query or feature).

 *To learn more about attention mechanisms, this page https://slds-lmu.github.io/seminar_nlp_ss20/attention-and-self-attention-for-nlp.html and 3blue1brown video https://www.3blue1brown.com/lessons/attentionare are helpful*

------
** 20240428
- paper: MEGALODON: Efficient LLM Pretraining and Inference with Unlimited Context Length
- links: https://arxiv.org/pdf/2404.08801
- ideas:
  1. traditional transformer: computation complexity, limited inductive bias.
  2. introduce the /complex exponential moving average(CEMA)/ components, timestamp normalization layer, normalized attention and pre-norm with two-hop residual configuraion.

     *Q1: This paper is based on the architecture of MEGA, But What is MEGA*?
     *Q2: Why this architecture and deal with unlimited length?*
     #+begin_quote
Evaluation on long-context modeling, including
perplexity in various context lengths up to 2M and long-context QA tasks in Scrolls (Parisotto et al.,

#+end_quote
*not understand*...
     
------
** 20240429
- paper: Mega: Moving Average Equipped Gated Attention
- links: https://arxiv.org/pdf/2209.10655
- ideas:
   1. sequence modeling common approaches: self-attention and EMA(exponential moving average)
     *Well, this kind of theortical paper is too difficult for me, mayme i should start with some basic ideas and understand the concepts by doing project.*
     
------
** 20240430
- paper: The Next Decade in AI
- links: https://arxiv.org/pdf/2002.06177
- ideas: 
  1. authors cites "The Bitter Lesson" - By Rich Sutton, i have seen this paper in many places. I should check out this paper.
  2. claim1: =/to build a robust, knowledge-driven approach to AI we must have the machinery of symbol-manipulation in our toolkit. Too much of useful knowledge is abstract to make do without tools that represent and manipulate abstraction, and to date, the only machinery that we know of that can manipulate such abstract knowledge reliably is the apparatus of symbol-manipulation/.=
  3. claim2: robust artificial intelligences properties:
     * have the ability to learn new knowledge
     * can learn knowledged that is symbolically represented.
     * significant knowledge is likely to be abstract.
     * rules and exceptions are co-existed
     * Some significant fraction of the knowledge that a robust system is likely to be causal, and to support counterfactuals.
     * Some small but important subset of human knowledge is likely to be innate; robust AI, too, should start with some important prior knowledge.
  4. claim3: rather than starting each new AI system from scratch, as a blank slate, with little knowledge of the world, we should seek to build learning systems that start with initial frameworks for domains like time, space, and causality, in order to speed up learning and massively constrain the hypothesis space.
  5. knowledge by itself it not enough. knowledge put into practice with tool of reasoning.
     #+begin_quote
a reasoning system that can leverage large-scale background knowledge
efficiently, even when available information is incomplete is a prerequisite to robustness.
#+end_quote

------
** 20240502
- paper: KAN: Kolmogorov–Arnold Networks
- links: https://arxiv.org/pdf/2404.19756
- ideas
  1. claim1: Kolmogorov-Arnold representation theorem
     *What is Kolmogorov-Arnold representation theorem? Why it can represented any function like Universal Approximation Theorem?* 
  2. claim2: MLP: learnable weights on edges, KAN learnable activation functions on edges.
     *TOMORRORW PAPER IS ABOUT UNIVERSAL APPROXIMATION THEOREM*
  3. claim3:  KANs’ nodes simply sum incoming signals without applying any non-linearities
  4. claim4:  KANs are nothing more than combinations of splines
     *What is splines?*
  5. claim5: Currently, the biggest bottleneck of KANs lies in its slow training. KANs are usually 10x slower than MLPs, given the same number of parameters. We should be honest that we did not try hard to optimize KANs’ efficiency though, so we deem KANs’ slow training more as an engineering problem to be improved in the future rather than a fundamental limitation. If one wants to train a model fast, one should use MLPs. In other cases, however, KANs should be comparable or better than MLPs, which makes them worth trying.  
------
------
** 20240503
- paper: Multilayer feedforward networks are universal approximators 
- links: https://cognitivemedium.com/magic_paper/assets/Hornik.pdf 
- ideas:
  1. claim1: Advocates of the virtues of multilayer feedfor- ward networks (e.g., Hecht-Nielsen, 1987) often cite /Kolmogorov’s/ (1957) superposition theorem or its more recent improvements (e.g.. Lorentz, 1976) in support of their capabilities. However, these results require a different unknown transformation (g in Lorentz’s notation) for each continuous function to be represented, while specifying an exact upper limit to the number of intermediate units needed for the representation.
  2. Anyway, this paper prove multilayer feedforward networks is a class of universal approximators.
     *While reading this paper, i am wondering why encoder-decoder structure network work? who proposed that? This is tomorrow topic.*


------
** 20240504
- paper: Sequence to Sequence Learning with Neural Networks 
- links: https://arxiv.org/pdf/1409.3215
- ideas:
  1. claim1:  DNNs can only be applied to problems whose inputs and targets can be sensibly encoded with vectors of fixed dimensionality.
  2. claim2:  network architecture,  one LSTM for encoder and another LSTM for decoder.
     *What is the encoder and the decoder has different network structure?*
------

** 20240505
- paper: Translating Videos to Natural Language Using Deep Recurrent Neural Networks 
- links: https://arxiv.org/pdf/1412.4729
- ideas:
  1. claim1: 
     ```
     video -> cnn -> lstm -> label
     ```

     *It seems like features extraction network is a kind of encoder-decoder structure networks.*
------

** 20240506
- paper: Summarizing Source Code using a Neural Attention Model 
- links: https://github.com/sriniiyer/codenn/blob/master/summarizing_source_code.pdf 
- ideas:
  1. claim1:
       dataset: stackoverflow that contains c# tag
       model: LSTM 

  *Today paper is about llm in code generation, i chose this paper from this slides https://webstanford.edu/class/cs224g/slides/Code%20Generation%20with%20LLMs.pdf and i discover ==Standford CS 224G=.= Great Resources for keeping track to frontier llm application.*


** 20240507
- paper: Learning to Control Fast-Weight Memories: An Alternative to Dynamic Recurrent Networks
- links: https://ieeexplore.ieee.org/document/6796337
- ideas:
  1. two feedword networks. first network produce "fast-weight" as short-term memory, memory controller 
     *Well, This URL is worth a look. https://people.idsia.ch//~juergen/most-cited-neural-nets.html*


** 20240508
- paper: Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention 
- links: https://arxiv.org/pdf/2006.16236
- ideas:
  1. claim1: tranditional transformers require quadratics memories, such as for $N$ input. The time complexity is $O(N_2)$. This paper propose linear transformation.
     *What's the difference between attention layer and self-attention layer?*
     #+begin_quote
every transformer can be seen as a recurrent neural network
#+end_quote




** 20240509
- paper: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
- links: https://arxiv.org/pdf/1810.04805 
- ideas:
  1. claim1: two methods for apply pre-trained language models to downstream tasks( feature-based and find-tuning)
     


** 20240510
- paper: Language Models are Unsupervised Multitask Learners
- links: https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf
- ideas:
  1. dataset WebText (millions of webpage)
  2. language modeling (unsupervised distribution estimation of examples, each example contains a length of symbols.)
        https://github.com/codelucas/newspaper
     


** 20240511
- paper: Improving Language Understanding by Generative Pre-Training
- links: https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf
- ideas:
  1. By pre-training on a diverse corpus with long stretches of contiguous text our model acquires significant world knowledge and ability to process long-range dependencies which are then successfully transferred to solving discriminative tasks such as question answering, semantic similarity assessment, entailmentdetermination, and text classification, improving the state of the art on 9 of the 12 datasets we study.

     *What sources of corpus is suitable for pre training?*



** 20240512
- paper: Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond
- links: https://arxiv.org/pdf/2304.13712
- ideas:
  *What's the difference between encoder-decoder structure with decoder only model?*
  1. decoder-only model
  2. nlu task: text classification, named entity recognition (NER),
entailment prediction, and so on. 
  3. nlg task: Natural Language Generation broadly encompasses two major categories of tasks, with the goal of creating coherent, meaningful, and contextually appropriate sequences of symbols. 
 

-------------
** 20240513
- paper: Cramming: Training a Language Model on a Single GPU in One Day
- links: https://arxiv.org/pdf/2212.14034
- ideas:
  no ideas here, i follow this https://magazine.sebastianraschka.com/p/understanding-large-language-models to read paper. But maybe one day this paper would be a good start for implementing BERT in consumer compute. I'm not sure if i will do this experiment. 


-------------
** 20240514
- paper: Autonomous LLM-driven research from data to human-verifiable research papers
- links: https://arxiv.org/pdf/2404.17605
- ideas:
  1. this paper propose data to paper. crazy idea...
  2. 
#+begin_quote
Starting with a human-provided dataset.the process is designed to raise hypotheses, write, debug and execute code to analyze the
data and perform statistical tests, interpret the results and write well-structured scientific
papers which not only describe results and conclusions but also transparently delineate the
research methodologies, allowing human scientists to understand, repeat and verify the
analysis. The discussion on emerging guidelines for AI-driven science (22) have served as a
design framework for data-to-paper, yielding a fully transparent, traceable and verifiable
workflow, and algorithmic \u201cchaining\u201d of data, methodology and result allowing to trace
downstream results back to the part of code which generated them. The system can run with
or without a predefined research goal (fixed/open-goal modalities) and with or without human
interactions and feedback (copilot/autopilot modes). We performed two open-goal and two
fixed-goal case studies on different public datasets (24\u201327) and evaluated the AI-driven
research process as well as the novelty and accuracy of created scientific papers. We show
that, running fully autonomously (autopilot), data-to-paper can perform complete and correct
run cycles for simple goals, while for complex goals, human co-piloting becomes critical.  
#+end_quote

-------------
** 20240515
- paper: LoRA: Low-Rank Adaptation of Large Language Models
- links: https://arxiv.org/pdf/2106.09685
- ideas:
  1. claim1: LoRA: which freezes the pre- trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable pa-rameters for downstream tasks
